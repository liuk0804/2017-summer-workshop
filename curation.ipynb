{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "curation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMOJRF43wa448DlWQ9Ja3Rw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liuk0804/2017-summer-workshop/blob/master/curation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7X4_XrRyc1E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "2f83dccb-fe95-4c30-d2ec-6cb03aa13048"
      },
      "source": [
        "!pip install ekphrasis\n",
        "!pip install pyenchant"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ekphrasis in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (2.0.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (4.41.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (3.2.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (3.2.1)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (0.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (1.18.4)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.6/dist-packages (from ekphrasis) (5.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->ekphrasis) (1.12.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ekphrasis) (0.10.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->ekphrasis) (0.1.9)\n",
            "Collecting pyenchant\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/66/9fe32edef9c56d9397ea7ab5853bc96082cda2770d3437ea0656758fd6d4/pyenchant-3.0.1-py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 1.9MB/s \n",
            "\u001b[?25hInstalling collected packages: pyenchant\n",
            "Successfully installed pyenchant-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5up-49n6f-P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "89e7c6fb-7839-481d-e260-8ce6db72db80"
      },
      "source": [
        "!yum install enchant "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: yum: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hyJ4U8eyFFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ekphrasis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DKtgsa7ylfH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer, Tokenizer\n",
        "\n",
        "import unicodedata"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeWbjHwZyl5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_--Risbcy-j4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import enchant\n",
        "d = enchant.Dict(\"en_US\")\n",
        "import string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9R-2lk9xzAS9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "b3a85989-6b4a-4835-989e-511bb331e4a5"
      },
      "source": [
        "social_pipeline = [\n",
        "        \"EMOJI\", \"URL\", \"TAG\", \"EMAIL\", \"USER\", \"HASHTAG\",\n",
        "        \"CASHTAG\", \"PHONE\", \"PERCENT\", \"MONEY\", \"DATE\", \"TIME\",\n",
        "        \"ACRONYM\", \"CENSORED\", \"EMPHASIS\", \"NUMBER\", \"WORD\" ]\n",
        "\n",
        "simple_proc = TextPreProcessor(tokenizer = Tokenizer().tokenize,)\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    fix_text = True, # fix unicode error\n",
        "\n",
        "    # pre-defined custom dictionary\n",
        "    dicts=[emoticons, slangdict],\n",
        "    \n",
        "    # replace token into <tag> using Regex pattern\n",
        "    normalize=['email', 'percent', 'money', 'phone', 'user',  'time', 'url', 'date', 'hashtag'],\n",
        "    # turn token into (word, <tag>) tuple\n",
        "    annotate= {'allcaps', 'elongated', 'repeated','emphasis', 'censored'},\n",
        "    \n",
        "    \n",
        "    # for word segmentation \n",
        "    segmenter=\"twitter\", \n",
        "    # for spell correction base on corpus statistics\n",
        "    corrector=\"twitter\", \n",
        "    \n",
        "    unpack_contractions= True,  # Unpack contractions (can't -> can not)\n",
        "    unpack_hashtags = False,\n",
        "    spell_correct_elong= True,  # spell correction for elongated words\n",
        "\n",
        "    tokenizer = SocialTokenizer(lowercase=True, pipeline = social_pipeline).tokenize,\n",
        ")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading english - 1grams ...\n",
            "Reading twitter - 1grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_1grams.txt\n",
            "You can't omit/backoff and unpack hashtags!\n",
            " unpack_hashtags will be set to False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlaGZTN3CQWi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c861c3d6-3ccb-49df-8bad-bf362aa989e3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMkJshifGCS3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f476beb9-8239-4648-876b-f1994333a707"
      },
      "source": [
        "!cat /content/drive/My\\ Drive/colab_file/"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat: '/content/drive/My Drive/colab_file/': Is a directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtfapzDSFdPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/drive/My Drive/colab_file/emoticon_lst_all.pkl', 'rb') as f:\n",
        "    emo_lst = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xPXrnvMH8mi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/drive/My Drive/colab_file/slang.pkl', 'rb') as f:\n",
        "    slang_lst = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjr569ipF72f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/drive/My Drive/colab_file/foo.txt', 'w') as f:\n",
        "  f.write('Hello Google Drive!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe5U7rX0GNXQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d54a01de-bb41-43dc-8b86-95df6796f4b0"
      },
      "source": [
        "!cat /content/drive/My\\ Drive/colab_file/foo.txt"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello Google Drive!"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS0rjX6qCXXK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def is_number(s):\n",
        "    try:\n",
        "        float(s)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        pass\n",
        " \n",
        "    try:\n",
        "        \n",
        "        unicodedata.numeric(s)\n",
        "        return True\n",
        "    except (TypeError, ValueError):\n",
        "        pass\n",
        " \n",
        "    return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIgBoeMTFcji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from content.drive.My\\ Drive.colab_file.emoticons import emoticons"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlAHuwXmQqLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/colab_file')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3btNLf7PRHea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from emoticons import emoticons\n",
        "from slangdict2 import slangdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVLDl73lROR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def en_word(word):\n",
        "    if is_number(word):\n",
        "        return False\n",
        "    elif word in string.punctuation:\n",
        "        return False\n",
        "    elif word in slang_lst:\n",
        "        return False\n",
        "    elif word in emo_lst:\n",
        "        return False\n",
        "    elif '<' in word:\n",
        "        return False\n",
        "    elif d.check(word):\n",
        "        return True\n",
        "    \n",
        "def not_en_word(word):\n",
        "    try:\n",
        "\n",
        "        if d.check(word):\n",
        "            return False\n",
        "        if is_number(word):\n",
        "            return False\n",
        "        elif word in string.punctuation:\n",
        "            return False\n",
        "        elif word in slang_lst:\n",
        "            return False\n",
        "        elif word in emo_lst:\n",
        "            return False\n",
        "        elif '<' in word:\n",
        "            return False\n",
        "        else:\n",
        "            return True\n",
        "    except ValueError:\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X3CJfZyRkHC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7f830753-cd9e-4411-c123-0cf7786e5d31"
      },
      "source": [
        "!apt-get install libenchant1c2a"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common enchant hunspell-en-us\n",
            "  libaspell15 libhunspell-1.6-0 libtext-iconv-perl\n",
            "Suggested packages:\n",
            "  aspell-doc spellutils wordlist hunspell openoffice.org-hunspell\n",
            "  | openoffice.org-core libenchant-voikko\n",
            "The following NEW packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common enchant hunspell-en-us\n",
            "  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n",
            "0 upgraded, 10 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 1,310 kB of archives.\n",
            "After this operation, 5,353 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtext-iconv-perl amd64 1.7-5build6 [13.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libaspell15 amd64 0.60.7~20110707-4ubuntu0.1 [309 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 emacsen-common all 2.0.8 [17.6 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 dictionaries-common all 1.27.2 [186 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 aspell amd64 0.60.7~20110707-4ubuntu0.1 [87.6 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 aspell-en all 2017.08.24-0-0.1 [298 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 hunspell-en-us all 1:2017.08.24 [168 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhunspell-1.6-0 amd64 1.6.2-1 [154 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libenchant1c2a amd64 1.6.0-11.1 [64.4 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 enchant amd64 1.6.0-11.1 [12.2 kB]\n",
            "Fetched 1,310 kB in 2s (565 kB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package libtext-iconv-perl.\n",
            "(Reading database ... 144429 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libtext-iconv-perl_1.7-5build6_amd64.deb ...\n",
            "Unpacking libtext-iconv-perl (1.7-5build6) ...\n",
            "Selecting previously unselected package libaspell15:amd64.\n",
            "Preparing to unpack .../1-libaspell15_0.60.7~20110707-4ubuntu0.1_amd64.deb ...\n",
            "Unpacking libaspell15:amd64 (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Selecting previously unselected package emacsen-common.\n",
            "Preparing to unpack .../2-emacsen-common_2.0.8_all.deb ...\n",
            "Unpacking emacsen-common (2.0.8) ...\n",
            "Selecting previously unselected package dictionaries-common.\n",
            "Preparing to unpack .../3-dictionaries-common_1.27.2_all.deb ...\n",
            "Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\n",
            "Unpacking dictionaries-common (1.27.2) ...\n",
            "Selecting previously unselected package aspell.\n",
            "Preparing to unpack .../4-aspell_0.60.7~20110707-4ubuntu0.1_amd64.deb ...\n",
            "Unpacking aspell (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Selecting previously unselected package aspell-en.\n",
            "Preparing to unpack .../5-aspell-en_2017.08.24-0-0.1_all.deb ...\n",
            "Unpacking aspell-en (2017.08.24-0-0.1) ...\n",
            "Selecting previously unselected package hunspell-en-us.\n",
            "Preparing to unpack .../6-hunspell-en-us_1%3a2017.08.24_all.deb ...\n",
            "Unpacking hunspell-en-us (1:2017.08.24) ...\n",
            "Selecting previously unselected package libhunspell-1.6-0:amd64.\n",
            "Preparing to unpack .../7-libhunspell-1.6-0_1.6.2-1_amd64.deb ...\n",
            "Unpacking libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Selecting previously unselected package libenchant1c2a:amd64.\n",
            "Preparing to unpack .../8-libenchant1c2a_1.6.0-11.1_amd64.deb ...\n",
            "Unpacking libenchant1c2a:amd64 (1.6.0-11.1) ...\n",
            "Selecting previously unselected package enchant.\n",
            "Preparing to unpack .../9-enchant_1.6.0-11.1_amd64.deb ...\n",
            "Unpacking enchant (1.6.0-11.1) ...\n",
            "Setting up libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Setting up libaspell15:amd64 (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Setting up emacsen-common (2.0.8) ...\n",
            "Setting up libtext-iconv-perl (1.7-5build6) ...\n",
            "Setting up dictionaries-common (1.27.2) ...\n",
            "Setting up aspell (0.60.7~20110707-4ubuntu0.1) ...\n",
            "Setting up hunspell-en-us (1:2017.08.24) ...\n",
            "Setting up libenchant1c2a:amd64 (1.6.0-11.1) ...\n",
            "Setting up aspell-en (2017.08.24-0-0.1) ...\n",
            "Setting up enchant (1.6.0-11.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for dictionaries-common (1.27.2) ...\n",
            "aspell-autobuildhash: processing: en [en-common].\n",
            "aspell-autobuildhash: processing: en [en-variant_0].\n",
            "aspell-autobuildhash: processing: en [en-variant_1].\n",
            "aspell-autobuildhash: processing: en [en-variant_2].\n",
            "aspell-autobuildhash: processing: en [en-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_AU-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_CA-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_US-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_US-wo_accents-only].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-HT1nSXRkwB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_OOV_token(oov_type, listOfElems):\n",
        "    try:\n",
        "        indexPosList = [ i-1 for i in range(len(listOfElems)) if listOfElems[i] == oov_type ]\n",
        "        return [listOfElems[index] for index in indexPosList]\n",
        "    except ValueError as e:\n",
        "        pass\n",
        "\n",
        "\n",
        "def Social_message_curation():\n",
        "    msg = input(\"Input a sample message: \") or  \"loovvvessss coofffeeeeee......\"    \n",
        "    print('')\n",
        "    print('start processing...')\n",
        "    simple_tok = simple_proc.pre_process_doc(msg)\n",
        "    tokenized = ' '.join(text_processor.pre_process_doc(msg)).lower().split(' ')\n",
        "    word_token = [i for i in tokenized if not (('<' in i) or (i in string.punctuation))]\n",
        "\n",
        "    print('Raw Message Inputed:', msg)\n",
        "    print('Standard NLP:', simple_tok)\n",
        "    print('Aggresive/Advanced NLP:', word_token)\n",
        "    print('OOV-curated NLP process:', tokenized)\n",
        "\n",
        "\n",
        "    df = pd.DataFrame({'message': [msg]})\n",
        "    df['tokenize'] = df.message.apply(text_processor.pre_process_doc)\\\n",
        "                               .apply(lambda x: ' '.join(x).lower().split(' '))\n",
        "    df['simple_tokenize'] = df.message.apply(simple_proc.pre_process_doc)\\\n",
        "                               .apply(lambda x: ' '.join(x).lower().split(' '))\n",
        "    df['en_IV'] = df.tokenize.apply(lambda x: list(filter(en_word, x)))\n",
        "    df['en_IV'] = df['en_IV'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
        "\n",
        "\n",
        "    # df['allcap_oov'] = df.tokenize.apply(lambda x: filter_OOV_token('<allcaps>', x))\n",
        "    df['slang_oov'] = df.tokenize.apply(lambda x: filter_OOV_token('<slang>', x))\n",
        "    df['emoji_oov']  = df.tokenize.apply(lambda x: filter_OOV_token('<emoji>', x))\n",
        "    df['elongated_oov']  = df.tokenize.apply(lambda x: filter_OOV_token('<elongated>', x))\n",
        "    df['repeated_punct']  = df.tokenize.apply(lambda x: filter_OOV_token('<repeated>', x))\n",
        "\n",
        "    df['misc_OOV'] = df.tokenize.apply(lambda x: list(filter(not_en_word, x)))\n",
        "\n",
        "    df2 = df[['en_IV','slang_oov','emoji_oov','elongated_oov','repeated_punct','misc_OOV']].transpose()\n",
        "    df2.columns = ['Captured_OOV']\n",
        "    df2['OOV_Count'] = df2.Captured_OOV.apply(len)\n",
        "\n",
        "    print(df2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "140-t06YR9gR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(language='english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5COPRuSOR06S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "cab3c621-cbb8-4296-bbda-872dfc5f3025"
      },
      "source": [
        "Social_message_curation()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input a sample message: \n",
            "\n",
            "start processing...\n",
            "Raw Message Inputed: loovvvessss coofffeeeeee......\n",
            "Standard NLP: ['loovvvessss', 'coofffeeeeee', '.', '.', '.', '.', '.', '.']\n",
            "Aggresive/Advanced NLP: ['love', 'coffee']\n",
            "OOV-curated NLP process: ['love', '<elongated>', 'coffee', '<elongated>', '.', '<repeated>']\n",
            "                  Captured_OOV  OOV_Count\n",
            "en_IV            [love, coffe]          2\n",
            "slang_oov                   []          0\n",
            "emoji_oov                   []          0\n",
            "elongated_oov   [love, coffee]          2\n",
            "repeated_punct             [.]          1\n",
            "misc_OOV                    []          0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb0wTpCXR2Zv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}